{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc68678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "import spacy\n",
    "from spacy.lang.sv.examples import sentences\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddaf2761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that reads .txt-files\n",
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f551b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file object variable (opening method will be rb)\n",
    "pdffileobj=open('personalhandboken.pdf','rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a35cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reader variable that will read the pdffileobj\n",
    "pdfreader = PyPDF2.PdfReader(pdffileobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d654bc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will store the number of pages of this pdf file\n",
    "x = len(pdfreader.pages)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e0be714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a variable that will select the selected number of pages\n",
    "page = pdfreader.pages[0]\n",
    "text = page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6da7cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_text = ''\n",
    "for i in range(0, x):\n",
    "    page = pdfreader.pages[i]\n",
    "    text = page.extract_text()\n",
    "    complete_text = complete_text + text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5945571",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_doc = complete_text.lower() # converting enitre text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7953e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download tokenizer packages for swedish\n",
    "nlp = spacy.load('sv_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "485f451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go trough the text. make it to lower case and remove punctuation like new lines, commas etc.\n",
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in ' \\n \\n  \\n\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_{|}~\\t\\n� ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b032cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = separate_punc(complete_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "637ecb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11629"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e84db73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequence of tokens\n",
    "# Give the network 25 words --> predict the next word (the #26 word)\n",
    "train_len = 25 + 1\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len, len(tokens)):\n",
    "    seq = tokens[i - train_len:i]\n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1dbc2ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\emch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # using the punkt tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33b1d487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\emch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet') # using the wordnet dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "440d0334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\emch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4') # using the punkt tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7a6c16cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = nltk.sent_tokenize(raw_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea6df386",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = nltk.word_tokenize(raw_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5fb7c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-processing\n",
    "lemmer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e997462",
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_tokens = []\n",
    "for token in tokens:\n",
    "    l = lemmer.lemmatize(token)\n",
    "    if token not in '  \\n \\n  \\n \\n \\n  \\n\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_{|}~\\t\\n�u ':\n",
    "        ready_tokens.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2fbf2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ce11e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punc_dict = dict((ord(punct), None) for punct in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9346f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "def LemNormalize(text):\n",
    "    test = LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))\n",
    "    print(test)\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "032cf9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "greet_inputs = ('hej', 'hejsan', 'hur är läget?', 'halloj?', 'Hej', 'Hejsan', 'Hur är läget?', 'Halloj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1c6a944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "greet_responses = ('Hej!', 'Hejsan!', 'Hej där!', 'Hej på dig!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f4c1b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greet(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in greet_inputs:\n",
    "            return random.choice(greet_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "07d89e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer # convert text to number (numeric representation)\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e6a8dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo1_response = ''\n",
    "    TfidfVec = TfidfVectorizer(tokenizer = LemNormalize, stop_words = 'english')\n",
    "    tfidf = TfidfVec.fit_transform(sentence_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf == 0):\n",
    "        robo1_response = robo1_response + 'Jag är ledsen. Jag är inte förmögen till att förstå din fråga!'\n",
    "        return robo1_response\n",
    "    else:\n",
    "        robo1_response = robo1_response + sentence_tokens[idx] + '\\n'\n",
    "        return robo1_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f225985e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hej! Jag är en bot som kan svara på frågor kring personalhandboken. Säg hej till mig för att starta igång mig. Skriv hej då för att avsluta.\n",
      "hej\n",
      "Bot: Hej!\n",
      "vad får man för ersättning på decerno?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emch\\AppData\\Local\\anaconda3\\envs\\cuda_environment\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abot', 'althogh', 'amont', 'arond', 'becase', 'bt', 'cold', 'coldnt', 'dring', 'enogh', 'fll', 'fond', 'frther', 'ha', 'herepon', 'hndred', 'le', 'mch', 'mst', 'n', 'nder', 'ntil', 'orselves', 'ot', 'p', 'pon', 'pt', 's', 'sch', 'serios', 'shold', 'th', 'therepon', 'thogh', 'thr', 'throgh', 'throghot', 'wa', 'wherepon', 'withot', 'wold', 'yo', 'yor', 'yors', 'yorself', 'yorselves'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Jag är ledsen. Jag är inte förmögen till att förstå din fråga!\n",
      "var är decerno?\n",
      "Bot: Jag är ledsen. Jag är inte förmögen till att förstå din fråga!\n"
     ]
    }
   ],
   "source": [
    "# Defining chatflow / botflow\n",
    "flag = True\n",
    "print('Hej! Jag är en bot som kan svara på frågor kring personalhandboken. Säg hej till mig för att starta igång mig. Skriv hej då för att avsluta.')\n",
    "while(flag == True):\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "    if(user_response != 'hej då'):\n",
    "        if(user_response == 'tack' or user_response == 'tackar'):\n",
    "            flag = False\n",
    "            print('Bot: You are welcome...')\n",
    "        else:\n",
    "            if(greet(user_response) != None):\n",
    "                print('Bot: ' + greet(user_response))\n",
    "            else:\n",
    "                sentence_tokens.append(user_response)\n",
    "                word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
    "                final_words = list(set(word_tokens))\n",
    "                print('Bot: ', end = '')\n",
    "                print(response(user_response))\n",
    "                sentence_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag = False \n",
    "        print('Bot: Hej då - på återseende!')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb541627",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa152af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e8801c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
